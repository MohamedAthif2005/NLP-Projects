{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1694baac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4369896",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Artificial intelligence is transforming the modern world. It is used in healthcare to assist doctors. AI helps detect diseases early. Machine learning is a core part of artificial intelligence. It allows systems to learn from data. Data quality is critical for machine learning models. Poor data leads to poor predictions. Deep learning is a subset of machine learning. It uses neural networks with multiple layers. These networks can model complex patterns. Image recognition has improved because of deep learning. Speech recognition systems are widely used today. Virtual assistants rely on natural language processing. NLP enables machines to understand human language. Text summarization is an NLP task. It helps reduce large documents into short summaries. Extractive summarization selects existing sentences. Abstractive summarization generates new sentences. Extractive methods are safer and simpler. TF-IDF is a common feature extraction technique. It assigns importance to words. Rare but frequent words get higher scores. Stop words are removed during preprocessing. Cosine similarity measures vector similarity. It focuses on direction rather than magnitude. Sentence similarity helps find central ideas. Central sentences represent the document theme. Ranking sentences is crucial for summarization. Sentence order affects readability. Sorting indices restores original flow. NLTK is a popular NLP library. It provides sentence tokenization. Tokenization splits text into units. Proper tokenization improves accuracy. Scikit-learn provides machine learning tools. It is widely used in industry. Python is the preferred language for NLP. It has strong community support. Data science relies heavily on Python. Automation improves productivity. AI systems can automate repetitive tasks. Ethical concerns exist in AI development. Bias in data can lead to unfair models. Transparency is important for trust. Explainable AI is an active research area. Governments are creating AI regulations. Responsible AI benefits society. Large datasets power modern models. Data storage has become cheaper. Cloud computing supports scalable AI. Model evaluation is essential. Overfitting reduces generalization. Validation data helps tune models. Hyperparameters affect performance. Optimization algorithms train models efficiently. Gradient descent is commonly used. Visualization helps understand data. Summaries save time for readers. Good summaries preserve meaning. Bad summaries lose context. Evaluation of summaries is challenging. Human judgment is often required. Research in NLP is growing rapidly. Future systems will be more capable.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0eea200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizer(text, ratio=0.2):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    total_sentences = len(sentences)\n",
    "    if total_sentences<3:\n",
    "        return sentences\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\",ngram_range=(1,2))\n",
    "    tfidf = vectorizer.fit_transform(sentences)\n",
    "    \n",
    "    similarity_matrix = cosine_similarity(tfidf)\n",
    "    sentence_scores = similarity_matrix.sum(axis=1)\n",
    "    \n",
    "    summary_size = int(total_sentences*0.3)\n",
    "    ranked_index = np.argsort(sentence_scores)[-summary_size:]\n",
    "    \n",
    "    ranked_index =sorted(ranked_index)\n",
    "    summary = \" \".join(sentences[i] for i in ranked_index)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5fe61a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI helps detect diseases early. Machine learning is a core part of artificial intelligence. It allows systems to learn from data. Data quality is critical for machine learning models. Deep learning is a subset of machine learning. Image recognition has improved because of deep learning. Speech recognition systems are widely used today. NLP enables machines to understand human language. Text summarization is an NLP task. It helps reduce large documents into short summaries. Extractive summarization selects existing sentences. Sentence similarity helps find central ideas. It provides sentence tokenization. Scikit-learn provides machine learning tools. Python is the preferred language for NLP. AI systems can automate repetitive tasks. Bias in data can lead to unfair models. Validation data helps tune models. Visualization helps understand data.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4fb01e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
